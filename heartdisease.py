# -*- coding: utf-8 -*-
"""HeartDisease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JSRZtqxkiwpcAP4ZwvXH-3JBCV56ctCa
"""

from google.colab import drive
drive.mount('/content/drive')

"""Import dependencies"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC



from sklearn.metrics import precision_score, recall_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
import xgboost as xgb
from sklearn.tree import DecisionTreeClassifier
#from catboost import CatBoostClassifier

import warnings

"""# **1. Data Preprocessing**"""

#loading the csv data to a pandas DataFrame
heart_data = pd.read_csv('/content/drive/MyDrive/heart/Heart_dataSet.csv')

#print fist 5 rows of the data set
heart_data.head()

#print last 5 rows of the data set
heart_data.tail()

#number of rows and columns in the data set
heart_data.shape

#getting some infor about data
heart_data.info()

"""**1.1 Data Cleaning**

Data cleaning is crucial because it lays the groundwork for accurate, reliable, and efficient data analysis and modeling, ultimately leading to better-informed decisions and insights.
"""

#checking for missing values
heart_data.isnull().sum()

#CHECK DUPLICATE VALUES
heart_data.duplicated().sum()

#drop the unneccary columns--> index and id
heart_data.drop(['index', 'id'], axis = 1, inplace = True)

# Check unique values of specified columns
colmuns_with_labels = ["gender", "cholesterol", "gluc", "smoke", "alco", "active"]

for i in colmuns_with_labels:
    x = heart_data[i].unique()
    x.sort()
    print(i + " has values: " + str(x))

# convert age into years
heart_data['age'] = heart_data['age'].apply(lambda x : int(x/365))

#convert weight in to int
heart_data['weight'] =heart_data['weight'].apply(lambda x : int(x))

#check outlier
heart_data_physio = heart_data[["height","weight","age","ap_hi", "ap_lo"]] # these variables have range
fig, axes = plt.subplots(3,2,figsize=(10,20))

ind = 1
for i in heart_data_physio.columns:
    if ind <= 6:
        plt.subplot(3,2,ind)
        sns.boxplot(data = heart_data_physio, y=i )
    ind = ind+1

#resolve outlier

heart_data = heart_data[(heart_data["ap_hi"] < 200) & (heart_data["ap_hi"] > 50) & (heart_data["ap_lo"] < 200) & (heart_data["ap_lo"] > 50)]

ind = 1
fig, axes = plt.subplots(3,2,figsize=(10,20))
heart_data_physio = heart_data[["height","weight","age","ap_hi", "ap_lo"]]

for i in heart_data_physio.columns:
    if ind <= 6:
        plt.subplot(3,2,ind)
        sns.boxplot(data = heart_data_physio, y=i )
    ind = ind+1

heart_data

#statistical measures about the data
heart_data.describe()

# checking the distributing of target variable
heart_data['cardio'].value_counts()

"""1--> Defective heart

0--> Healthy Haart
"""

heart_data.head()

"""**1.2 Data Analysis**"""

# Transformed the values in specific columns to more interpretable labels or categories as it makes the
# data more human-readable and facilitates better understanding during data analysis and exploration.

heart_data_analyze = heart_data.copy()
heart_data_analyze["gender"]= heart_data_analyze["gender"].apply(lambda x: "male" if x == 1 else "female")
heart_data_analyze["alco"]= heart_data_analyze["alco"].apply(lambda x: "No" if x == 0 else "Yes")
heart_data_analyze["smoke"]= heart_data_analyze["smoke"].apply(lambda x: "No" if x == 0 else "Yes")
heart_data_analyze["active"]= heart_data_analyze["active"].apply(lambda x: "No" if x == 0 else "Yes")

def transform_value(val):
    if val == 1:
        return "type 1"
    elif val == 2:
        return "type 2"
    else:
        return "type 3"

heart_data_analyze['cholesterol']= heart_data_analyze['cholesterol'].apply(transform_value)
heart_data_analyze['gluc']= heart_data_analyze['gluc'].apply(transform_value)

heart_data_analyze.head()
heart_data_test = heart_data_analyze

"""**Distribution of cardiovascular disease**"""

plt.figure(figsize = (10, 8))
plt.title('Cardiovascular Disease')
plt.pie(heart_data_analyze['cardio'].value_counts(), labels = ['No Cardiovascular Disease', 'Cardiovascular Disease'], explode = (0.1, 0.0), colors = ['lightblue', 'orange'],autopct = '%1.2f%%', shadow = True)
plt.legend(loc = 'best')

"""**Distribution of values for categorical features**"""

plt.figure(figsize=(30, 50))

ind = 0
for i in colmuns_with_labels:
    ind = ind + 1
    plt.subplot(7, 2, ind)

    plt.pie(heart_data_analyze[i].value_counts(), labels=heart_data_analyze[i].unique(), autopct='%1.1f%%', colors=sns.color_palette('pastel'), startangle=90)

    plt.axis('equal')
    plt.title(i)

plt.tight_layout()
plt.show()

"""**Distribution of cardio disease for age groups**"""

plt.rcParams['figure.figsize'] = 11, 8
sns.countplot(x='age', hue='cardio', data = heart_data_analyze, palette="Set1");

heart_data_physio  # give the  discribution details
#given only the column we got for the  check outiers
# check whetther have cardio or not

"""**1.3 Feature Selection and Data Transformation**"""

#BMI, risk level scores, and pulse pressure are useful features in healthcare-related datasets as they provide insights into factors affecting cardiovascular health.
heart_data['pp'] = heart_data['ap_hi']-heart_data['ap_lo']  #pp = pulse pressure
heart_data["bmi"] = heart_data["weight"]/((heart_data["height"]/100)*(heart_data["height"]/100))

"""Health Risk Metric

Cholesterol Level: Weight = 3 (higher weight because cholesterol is a significant health risk factor).

Smoking Status: Weight = 3 (higher weight due to the strong association with health risks).

Glucose Level: Weight = 2 (moderate weight as elevated glucose levels are concerning).

Alcohol Consumption: Weight = 1 (lower weight as it has a less direct impact compared to the other factors).
"""

weights = {
    'Chol': 3,
    'Smoke': 3,
    'Gluc': 2,
    'Alco': 1
}

heart_data['health_risk_score'] = heart_data['cholesterol'] * weights['Chol'] + \
                           heart_data['gluc'] * weights['Gluc'] + \
                           heart_data['smoke'] * weights['Smoke'] + \
                           heart_data['alco'] * weights['Alco']

# computes and stores the correlation matrix for the columns in the heart_data dataset.
heart_data_corr = heart_data.corr()
heart_data_corr

mask = np.triu(heart_data_corr)
plt.figure(figsize = (10, 7))
plt.title("Correlation Matrix")
sns.heatmap(heart_data_corr, cmap = 'viridis', annot = True, mask = mask, linecolor = 'white', linewidth = 0.5, fmt = '.2f')

"""Drop features with the absulute value of correlation with cardio < 0.1"""

heart_data = heart_data.drop(["gender","height","smoke","alco","gluc","active"], axis="columns")

heart_data.head()

heart_data.tail()

"""# 2. Model Building and Evaluation

1. Logistic Regression

Split in to test and train data
"""

X = heart_data.drop(columns='cardio', axis=1)
Y = heart_data['cardio']

print(X)

print(Y)

# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=2)

# print(X.shape, X_train.shape, X_test.shape)

# model = LogisticRegression()

#training the LogisticRegression model with training data
#model.fit(X_train, Y_train)

"""Model evaluation
Accuracy score
"""

# Accuracy on training data
# X_train_prediction = model.predict(X_train)
# training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

# print('Accuracy on Training data: ',training_data_accuracy)

# Accuracy on test data
# X_test_prediction = model.predict(X_test)
# test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

# print('Accuracy on Test data: ',test_data_accuracy)

"""Training the model using a for loop"""

# Split the data
# X = heart_data.drop('cardio', axis=1)
# y = heart_data['cardio']
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

#Initialize the model1
model1 = LogisticRegression(max_iter=1000)  # Adjust max_iter as needed

# Lists to store training and testing metrics
train_accuracy = []
test_accuracy = []
train_loss = []
test_loss = []
precision_scores = []
recall_scores = []

# Train the model1 and collect metrics
for i in range(100):  # You can adjust the number of iterations
    model1.fit(X_train, y_train)

    # Training accuracy
    X_train_prediction = model1.predict(X_train)
    training_data_accuracy = accuracy_score(X_train_prediction, y_train)
    train_accuracy.append(training_data_accuracy)

    # Testing accuracy
    y_pred = model1.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    test_accuracy.append(accuracy)

    # Precision and Recall
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    precision_scores.append(precision)
    recall_scores.append(recall)

    # You can also add code here to collect loss values if the XGBoost library you're using provides them.
    # For example, you can collect them after each boosting round.

print('Accuracy on Training data: ',training_data_accuracy)
print("Accuracy on Testing data:", accuracy)
print("precission",precision)
print("recall : ",recall)

# Create a plot
plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 1)
plt.plot(train_accuracy, label='Train Accuracy', marker='o')
plt.plot(test_accuracy, label='Test Accuracy', marker='o')
plt.xlabel('Number of Iterations')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(precision_scores, label='Precision', marker='o', color='g')
plt.plot(recall_scores, label='Recall', marker='o', color='b')
plt.xlabel('Number of Iterations')
plt.ylabel('Score')
plt.title('Precision and Recall')
plt.legend()

plt.subplot(1, 3, 3)
# Plot train and test loss if available

# Display the confusion matrix
cm_LogisticRegression = confusion_matrix(y_test, y_pred)
sns.heatmap(cm_LogisticRegression, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted Labels')
plt.title('Confusion Matrix')

plt.show()

"""2. XGB ** *italicized text*algorithm**"""

# Split the data
X = heart_data.drop('cardio', axis=1)
y = heart_data['cardio']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model2
model2 = xgb.XGBClassifier()

# Lists to store training and testing metrics
train_accuracy = []
test_accuracy = []
train_loss = []
test_loss = []
precision_scores = []
recall_scores = []

# Train the model2 and collect metrics
for i in range(100):  # You can adjust the number of iterations
    model2.fit(X_train, y_train)

    # Training accuracy
    X_train_prediction = model2.predict(X_train)
    training_data_accuracy = accuracy_score(X_train_prediction, y_train)
    train_accuracy.append(training_data_accuracy)

    # Testing accuracy
    y_pred = model2.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    test_accuracy.append(accuracy)

    # Precision and Recall
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    precision_scores.append(precision)
    recall_scores.append(recall)

    # You can also add code here to collect loss values if the XGBoost library you're using provides them.
    # For example, you can collect them after each boosting round.

print('Accuracy on Training data: ',training_data_accuracy)
print("Accuracy on Testing data:", accuracy)
print("precission",precision)
print("recall : ",recall)

# Create a plot
plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 1)
plt.plot(train_accuracy, label='Train Accuracy', marker='o')
plt.plot(test_accuracy, label='Test Accuracy', marker='o')
plt.xlabel('Number of Iterations')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(precision_scores, label='Precision', marker='o', color='g')
plt.plot(recall_scores, label='Recall', marker='o', color='b')
plt.xlabel('Number of Iterations')
plt.ylabel('Score')
plt.title('Precision and Recall')
plt.legend()

plt.subplot(1, 3, 3)
# Plot train and test loss if available

# Display the confusion matrix
cm_xgb = confusion_matrix(y_test, y_pred)
sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted Labels')
plt.title('Confusion Matrix')

plt.show()

"""3. Decision Tree"""

# Split the data
X = heart_data.drop('cardio', axis=1)
y = heart_data['cardio']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Initialize the model3
model3 = DecisionTreeClassifier()

#Lists to store training and testing metrics

train_accuracy = []
test_accuracy = []
train_loss = []
test_loss = []
precision_scores = []
recall_scores = []

# Train the model3 and collect metrics
for i in range(100):  # You can adjust the number of iterations
    model3.fit(X_train, y_train)

    # Training accuracy
    X_train_prediction = model3.predict(X_train)
    training_data_accuracy = accuracy_score(X_train_prediction, y_train)
    train_accuracy.append(training_data_accuracy)

    # Testing accuracy
    y_pred = model3.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    test_accuracy.append(accuracy)

    # Precision and Recall
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    precision_scores.append(precision)
    recall_scores.append(recall)

    # You can also add code here to collect loss values if the XGBoost library you're using provides them.
    # For example, you can collect them after each boosting round.

print('Accuracy on Training data: ',training_data_accuracy)
print("Accuracy on Testing data:", accuracy)
print("precission",precision)
print("recall : ",recall)

# Create a plot
plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 1)
plt.plot(train_accuracy, label='Train Accuracy', marker='o')
plt.plot(test_accuracy, label='Test Accuracy', marker='o')
plt.xlabel('Number of Iterations')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(precision_scores, label='Precision', marker='o', color='g')
plt.plot(recall_scores, label='Recall', marker='o', color='b')
plt.xlabel('Number of Iterations')
plt.ylabel('Score')
plt.title('Precision and Recall')
plt.legend()

plt.subplot(1, 3, 3)
# Plot train and test loss if available

# Display the confusion matrix
cm_tree = confusion_matrix(y_test, y_pred)
sns.heatmap(cm_tree, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted Labels')
plt.title('Confusion Matrix')

plt.show()

"""4. Support Vector Classifier"""

from sklearn.svm import SVC
import numpy as np

# Split the data
X = heart_data.drop('cardio', axis=1)
y = heart_data['cardio']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model4
model4 = SVC()

# Lists to store training and testing metrics
train_accuracy = []
test_accuracy = []
precision_scores = []
recall_scores = []

# Train the model4 for a single iteration
model4.fit(X_train, y_train)

# Training accuracy
X_train_prediction = model4.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, y_train)
train_accuracy.append(training_data_accuracy)

# Testing accuracy
y_pred = model4.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
test_accuracy.append(accuracy)

# Precision and Recall
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision_scores.append(precision)
recall_scores.append(recall)


# Create a plot
plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 1)
plt.plot(train_accuracy, label='Train Accuracy', marker='o')
plt.plot(test_accuracy, label='Test Accuracy', marker='o')
plt.xlabel('Number of Iterations')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(precision_scores, label='Precision', marker='o', color='g')
plt.plot(recall_scores, label='Recall', marker='o', color='b')
plt.xlabel('Number of Iterations')
plt.ylabel('Score')
plt.title('Precision and Recall')
plt.legend()

plt.subplot(1, 3, 3)
# Plot train and test loss if available

# Display the confusion matrix
cm_svc = confusion_matrix(y_test, y_pred)
sns.heatmap(cm_svc, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted Labels')
plt.title('Confusion Matrix')

plt.show()

print('Accuracy on Training data: ',training_data_accuracy)
print("Accuracy on Testing data:", accuracy)
print("precission",precision)
print("recall : ",recall)

"""Comparing results form all models

"""

tn_lg, fp_lg, fn_lg, tp_lg = cm_LogisticRegression.ravel()
tn_xgb, fp_xgb, fn_xgb, tp_xgb = cm_xgb.ravel()
tn_tree, fp_tree, fn_tree, tp_tree = cm_tree.ravel()
tn_svc, fp_svc, fn_svc, tp_svc = cm_svc.ravel()

fn_values = [fn_lg, fn_xgb, fn_tree, fn_svc]
fp_values = [fp_lg, fp_xgb, fp_tree, fp_svc]

total_values = [fn + fp for fn, fp in zip(fn_values, fp_values)]

model_names = ["LogisticRegression", "XGBoost", "DecisionTree","SVC"]

x = np.arange(len(model_names))
custom_labels = [f"LogisticRegression \nIncorrectly forecasted total \nvalue: ({total_values[0]})",
                 f"XGBoost  \nIncorrectly forecasted total \nvalue:({total_values[1]})\n",
                 f"DecisionTree  \nIncorrectly forecasted total \nvalue:({total_values[2]})\n",
                 f"SVC  \nIncorrectly forecasted total \nvalue:({total_values[3]})"]


fn_color = 'green'
fp_color = 'yellow'

bar_width = 0.35
fig, ax = plt.subplots()
bar1 = ax.bar(x - bar_width / 2, fn_values, width=bar_width, color=fn_color, label='False Negatives')
bar2 = ax.bar(x + bar_width / 2, fp_values, width=bar_width, color=fp_color, label='False Positives')

ax.set_xticks(x)
ax.set_xticklabels(custom_labels)

plt.xlabel('Algorithms')
plt.ylabel('Counts')
plt.title('False Negatives and False Positives Comparison Among Algorithms')

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.legend()

for bar1, bar2, fn_value, fp_value in zip(bar1, bar2, fn_values, fp_values):
    ax.text(bar1.get_x() + bar1.get_width() / 2 - 0.08, fn_value + 5, str(fn_value), ha='center', fontsize=10, color='black')
    ax.text(bar2.get_x() + bar2.get_width() / 2 - 0.08, fp_value + 5, str(fp_value), ha='center', fontsize=10, color='black')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have the confusion matrix values for each model
tn_lg, fp_lg, fn_lg, tp_lg = cm_LogisticRegression.ravel()
tn_xgb, fp_xgb, fn_xgb, tp_xgb = cm_xgb.ravel()
tn_tree, fp_tree, fn_tree, tp_tree = cm_tree.ravel()
tn_svc, fp_svc, fn_svc, tp_svc = cm_svc.ravel()

# Calculate precision and recall for each model
precision_lg = tp_lg / (tp_lg + fp_lg) if (tp_lg + fp_lg) > 0 else 0
recall_lg = tp_lg / (tp_lg + fn_lg) if (tp_lg + fn_lg) > 0 else 0

precision_xgb = tp_xgb / (tp_xgb + fp_xgb) if (tp_xgb + fp_xgb) > 0 else 0
recall_xgb = tp_xgb / (tp_xgb + fn_xgb) if (tp_xgb + fn_xgb) > 0 else 0

precision_tree = tp_tree / (tp_tree + fp_tree) if (tp_tree + fp_tree) > 0 else 0
recall_tree = tp_tree / (tp_tree + fn_tree) if (tp_tree + fn_tree) > 0 else 0

precision_svc = tp_svc / (tp_svc + fp_svc) if (tp_svc + fp_svc) > 0 else 0
recall_svc = tp_svc / (tp_svc + fn_svc) if (tp_svc + fn_svc) > 0 else 0

# Assuming you have lists to store these precision and recall values
precision_scores = [precision_lg, precision_xgb, precision_tree, precision_svc]
recall_scores = [recall_lg, recall_xgb, recall_tree, recall_svc]


model_names = ["LogisticRegression", "XGBoost", "DecisionTree", "SVC"]

x = np.arange(len(model_names))
custom_labels = [f"{model} \nPrecision: {precision:.2f}, Recall: {recall:.2f}" for model, precision, recall in zip(model_names, precision_scores, recall_scores)]

precision_color = 'blue'
recall_color = 'orange'

bar_width = 0.35
fig, ax = plt.subplots()
bar1 = ax.bar(x - bar_width / 2, precision_scores, width=bar_width, color=precision_color, label='Precision')
bar2 = ax.bar(x + bar_width / 2, recall_scores, width=bar_width, color=recall_color, label='Recall')

ax.set_xticks(x)
ax.set_xticklabels(custom_labels)

plt.xlabel('Algorithms')
plt.ylabel('Scores')
plt.title('Precision and Recall Comparison Among Algorithms')

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.legend()

for bar1, bar2, precision, recall in zip(bar1, bar2, precision_scores, recall_scores):
    ax.text(bar1.get_x() + bar1.get_width() / 2 - 0.08, precision + 0.02, f'{precision:.2f}', ha='center', fontsize=10, color='black')
    ax.text(bar2.get_x() + bar2.get_width() / 2 - 0.08, recall + 0.02, f'{recall:.2f}', ha='center', fontsize=10, color='black')

plt.tight_layout()
plt.show()

"""Hyper Parameter tunning for XGBoost model"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns


# Split the data
X = heart_data.drop('cardio', axis=1)
y = heart_data['cardio']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hyperparameter Tuning using GridSearchCV
param_grid = {
    'learning_rate': [0.1, 0.01],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

model = xgb.XGBClassifier()
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)
grid_result = grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_result.best_params_
print("Best Parameters:", best_params)

# Model Training and Metrics Collection
model2 = xgb.XGBClassifier(**best_params)  # Using the best parameters from GridSearchCV

# Lists to store training and testing metrics
train_accuracy = []
test_accuracy = []
precision_scores = []
recall_scores = []

# Train the model and collect metrics
for i in range(100):  # You can adjust the number of iterations
    model2.fit(X_train, y_train)

    # Training accuracy
    X_train_prediction = model2.predict(X_train)
    training_data_accuracy = accuracy_score(X_train_prediction, y_train)
    train_accuracy.append(training_data_accuracy)

    # Testing accuracy
    y_pred = model2.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    test_accuracy.append(accuracy)

    # Precision and Recall
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    precision_scores.append(precision)
    recall_scores.append(recall)

print("Accuracy on Testing data:", accuracy)
print("Precision:", precision)
print("Recall : ", recall)

# Create a plot
plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
plt.plot(train_accuracy, label='Train Accuracy', marker='o')
plt.plot(test_accuracy, label='Test Accuracy', marker='o')
plt.xlabel('Number of Iterations')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(precision_scores, label='Precision', marker='o', color='g')
plt.plot(recall_scores, label='Recall', marker='o', color='b')
plt.xlabel('Number of Iterations')
plt.ylabel('Score')
plt.title('Precision and Recall')
plt.legend()

plt.subplot(1, 3, 3)
# Plot train and test loss if available

# Display the confusion matrix
cm_xgb = confusion_matrix(y_test, y_pred)
sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted Labels')
plt.title('Confusion Matrix')

plt.show()

# increase testiing accuracy after using hyper parameaters

"""# 3. Model Deployment

Saving the Selected trained model
"""

import pickle

filename = 'trained_model.sav'

#writing the model in binary format
pickle.dump(model2, open(filename, 'wb'))

#loading the saved model
loaded_model = pickle.load(open('trained_model.sav', 'rb')) #reading the binary format

"""**Building a Predictive System**"""

#input_data
input_data = (50,1,158,72,120,80,1,1,0,0,1)

# Unpack the tuple into individual variables
age, gender, height, weight, ap_hi, ap_lo, cholesterol, gluc, smoke, alco, active = input_data

# Convert age from years to days
age_in_days = age * 365  # Assuming an average year has 365 days

# BMI and pulse pressure
pp = ap_hi - ap_lo
bmi = weight / ((height / 100) * (height / 100))

# Calculating health risk score
weights = {
    'Chol': 3,
    'Smoke': 3,
    'Gluc': 2,
    'Alco': 1
}

health_risk_score = cholesterol * weights['Chol'] + gluc * weights['Gluc'] + smoke * weights['Smoke'] + alco * weights['Alco']

# Create modified input data as a tuple
modified_input_data = (age_in_days, weight, ap_hi, ap_lo, cholesterol, pp, bmi, health_risk_score)

# Display the modified input data
print(modified_input_data)

#Change the input data to a numpy array
input_data_as_numpy_array = np.asarray(modified_input_data)

# Reshape the numpy array as we are predicting for only on instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

#Select the model
selected_model = loaded_model

prediction = selected_model.predict(input_data_reshaped)
print(prediction)

if (prediction[0]==0):
  print('The Person does not have a Heart Disease')
else:
  print('The Person has a Heart Disease')

pip install shap

import shap

user_df = pd.DataFrame(input_data_reshaped, columns=['age', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'pp','bmi','health_risk_score' ])

# Initialize an explainer with the loaded model
explainer = shap.Explainer(loaded_model)

# Calculate SHAP values for the user's data
shap_values = explainer.shap_values(user_df)

# Plot the summary plot
shap.summary_plot(shap_values, user_df, plot_type="bar", show=False)
plt.title("Feature Importance for Heart Disease Prediction")
plt.show()